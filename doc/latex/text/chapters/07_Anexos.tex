\chapter{Anexos}
\label{chap:anexos}

\section{Anexo 1. Salida de la consola tras ejecutar la primera versión}
\label{sec:v1-output}

\begin{lstlisting}[language=bash]
/usr/bin/python3 src/mnist.py
Using TensorFlow backend.
Using TensorFlow v1.12.0
The number of occuranc of each number in the train set is {0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}

The number of occuranc of each number in the test set is {0: 980, 1: 1135, 2: 1032, 3: 1010, 4: 982, 5: 892, 6: 958, 7: 1028, 8: 974, 9: 1009}

Train on 60000 samples, validate on 60000 samples
Epoch 1/15
2018-11-22 23:14:01.439601: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
60000/60000 [==============================] - 2s 27us/step - loss: 0.3603 - acc: 0.9007 - val_loss: 0.1898 - val_acc: 0.9472
Epoch 2/15
60000/60000 [==============================] - 1s 24us/step - loss: 0.1674 - acc: 0.9526 - val_loss: 0.1290 - val_acc: 0.9639
Epoch 3/15
60000/60000 [==============================] - 1s 25us/step - loss: 0.1205 - acc: 0.9656 - val_loss: 0.0996 - val_acc: 0.9711
Epoch 4/15
60000/60000 [==============================] - 1s 25us/step - loss: 0.0929 - acc: 0.9736 - val_loss: 0.0774 - val_acc: 0.9777
Epoch 5/15
60000/60000 [==============================] - 1s 24us/step - loss: 0.0750 - acc: 0.9784 - val_loss: 0.0606 - val_acc: 0.9826
Epoch 6/15
60000/60000 [==============================] - 1s 24us/step - loss: 0.0622 - acc: 0.9819 - val_loss: 0.0499 - val_acc: 0.9866
Epoch 7/15
60000/60000 [==============================] - 1s 25us/step - loss: 0.0524 - acc: 0.9853 - val_loss: 0.0417 - val_acc: 0.9884
Epoch 8/15
60000/60000 [==============================] - 2s 25us/step - loss: 0.0439 - acc: 0.9877 - val_loss: 0.0369 - val_acc: 0.9902
Epoch 9/15
60000/60000 [==============================] - 1s 25us/step - loss: 0.0375 - acc: 0.9895 - val_loss: 0.0288 - val_acc: 0.9930
Epoch 10/15
60000/60000 [==============================] - 1s 24us/step - loss: 0.0326 - acc: 0.9911 - val_loss: 0.0243 - val_acc: 0.9942
Epoch 11/15
60000/60000 [==============================] - 1s 24us/step - loss: 0.0271 - acc: 0.9927 - val_loss: 0.0218 - val_acc: 0.9952
Epoch 12/15
60000/60000 [==============================] - 1s 24us/step - loss: 0.0240 - acc: 0.9939 - val_loss: 0.0168 - val_acc: 0.9963
Epoch 13/15
60000/60000 [==============================] - 2s 26us/step - loss: 0.0209 - acc: 0.9946 - val_loss: 0.0161 - val_acc: 0.9966
Epoch 14/15
60000/60000 [==============================] - 2s 26us/step - loss: 0.0174 - acc: 0.9958 - val_loss: 0.0128 - val_acc: 0.9973
Epoch 15/15
60000/60000 [==============================] - 2s 25us/step - loss: 0.0149 - acc: 0.9965 - val_loss: 0.0114 - val_acc: 0.9980
Training time: 22.613s
60000/60000 [==============================] - 1s 22us/step
Train loss: 0.01141
Train accuracy: 99.80500%
10000/10000 [==============================] - 0s 23us/step
Test loss: 0.07754
Test accuracy: 97.62000%

\end{lstlisting}

\newpage
\section{Anexo 2. Estructura de directorios del proyecto}

\begin{itemize}
  \item El direcotrio \textit{deliverables/} contiene los resultados de cada versión del proyecto.
  \item La carpeta \textit{doc/} contiene la documentación final y el guión de la práctica.
  \item Los archivo \textit{LICENSE} y \textit{README.txt} son archivos de configuración del repositorio.
  \item El archivo \textit{Makefile} sirve para instalar y ejecutar el proyecto.
  \item El archivo \textit{requirements.txt} contiene los nombres y las versiones de las librerías utilizadas en el proyecto, y es usado por la herramienta pip para instalarlas.
  \item El directorio \textit{src/} contiene los archivos de código fuente de la práctica.
  \begin{itemize}
    \item \textit{aux.py} contiene funciones auxiliares utilizados para renderizar las imágenes de la base de datos.
    \item \textit{mist\_functions.py} contiene las llamadas a la API de Keras envueltas en funciones personalizadas ya que, en un intento por aumentar la claridad y la mantenibilidad del proyecto, el archivo principal llama a estas funciones.
    \item \textit{mnist.py} es el archivo principal y contiene la inicialización de las librerías, la configuración de las capas de la red neuronal y las llamadas a las funciones de \textit{mnist\_functions.py}.
  \end{itemize}
\end{itemize}


\newpage
\section{Anexo 3. Código fuente de la práctica}

\subsection{mnist.py}

\begin{lstlisting}[language=python]
#!/usr/bin/python3
# -*- coding:utf-8 -*-

# TensorFlow and tf.keras
import tensorflow as tf
from tensorflow import keras
from keras import backend
print("Using TensorFlow v{}".format(tf.__version__))

#Local libraries
from aux import *
from mnist_functions import *

# Downloading MNIST dataset from keras
(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()

# Getting unique values from labels, sorting them and storing
class_names = list(set(train_labels))
num_classes = len(class_names)
input_shape = (28, 28, 1) # 28x28 pixels, no z coordinate

# Printing how manny different numbers are in each set
print_unique_numbers('train', train_labels)
print_unique_numbers('test', test_labels)

# Prepocessing
## Normalising datasets before feeding to the neural network
train_images = train_images / 255.0
test_images = test_images / 255.0

train_images = train_images.reshape(train_images.shape[0], 28, 28, 1)
train_images = train_images.astype('float32')

test_images = test_images.reshape(test_images.shape[0], 28, 28, 1)
test_images = test_images.astype('float32')

## convert class vectors to binary class matrices
train_labels = keras.utils.to_categorical(train_labels, num_classes)
test_labels = keras.utils.to_categorical(test_labels, num_classes)

# Building the model
model = keras.Sequential()
model.add(keras.layers.Conv2D(32, kernel_size=(4, 4),
activation=tf.nn.relu,
input_shape=input_shape))
model.add(keras.layers.Conv2D(64, (3, 3), activation=tf.nn.relu))
model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))
model.add(keras.layers.Dropout(0.2))
model.add(keras.layers.Flatten())
model.add(keras.layers.Dense(248, activation=tf.nn.relu))
model.add(keras.layers.Dense(124, activation=tf.nn.relu))
model.add(keras.layers.Dropout(0.4))
model.add(keras.layers.Dense(num_classes, activation=tf.nn.softmax))

# Compiling the model
model.compile(optimizer=keras.optimizers.Adam(),
loss=keras.losses.categorical_crossentropy,
metrics=['accuracy'])

# Train the model with the train images
EPOCHS = 20

train_time_str = train_model(model, train_images, train_labels, EPOCHS)

# Evaluate the accuracy of the model with the train images
train_loss_str, train_acc_str = evaluate_model('Train', model, train_images, train_labels)

# Evaluate the accuracy of the model with the test images
test_loss_str, test_acc_str = evaluate_model('Test', model, test_images, test_labels)

# Save results
VERSION = 3
save_results(VERSION, train_time_str, train_loss_str, train_acc_str, test_loss_str, test_acc_str)

# Predicting the labels of all train images
predict_labels(VERSION, 'train', model, train_images)

# Predicting the labels of all test images
predict_labels(VERSION, 'test', model, test_images)

save_model_layers_to_file(VERSION, 'model.png', model)
\end{lstlisting}


\subsection{mist\_functions.py}

\begin{lstlisting}[language=python]
#!/usr/bin/python3
# -*- coding:utf-8 -*-

import time
import os
import numpy
from keras.utils import plot_model

def print_unique_numbers(TYPE, labels):
    unique, count= numpy.unique(labels, return_counts=True)
    print("The number of occurrence of each number in the {} set is {}\n".format(TYPE, dict (zip(unique, count))))


def train_model(model, images, labels, EPOCHS):
    start_time = time.time()

    model.fit(images, labels, epochs=EPOCHS, batch_size=128, validation_data=(images, labels))

    end_time = time.time()
    elapsed_time = end_time - start_time
    elapsed_time_str = "Training time: {:.3f}s".format(elapsed_time)
    print(elapsed_time_str)

    return elapsed_time_str

def evaluate_model(TYPE, model, images, labels):
    loss, acc = model.evaluate(images, labels)
    loss_str = '{0} loss: {1:.5f}'.format(TYPE, loss)
    acc_str = '{0} accuracy: {1:.5f}%'.format(TYPE, acc*100)
    print(loss_str)
    print(acc_str)

    return (loss_str, acc_str)

def save_results(VERSION, train_time, train_loss, train_acc, test_loss, test_acc):
    dir_name = 'deliverables/ver_{0}/'.format(VERSION)
    if not os.path.exists(dir_name):
        os.makedirs(dir_name)

    filename = dir_name + 'result.txt'
    with open(filename, 'w') as f_out:
        f_out.write(train_time + '\n')
        f_out.write(train_loss + '\n')
        f_out.write(train_acc + '\n')
        f_out.write(test_loss + '\n')
        f_out.write(test_acc + '\n')

def predict_labels(VERSION, TYPE, model, images):
    predictions = model.predict(images)
    assigned_labels = numpy.argmax(predictions, axis=-1)

    assert len(predictions) == len(images)

    filename = 'deliverables/ver_{0}/assigned_labels_{1}.txt'.format(VERSION, TYPE)
    with open(filename, 'w') as f_out:
        for label in assigned_labels:
            f_out.write(str(label))

    return assigned_labels

def save_model_layers_to_file(VERSION, name, model):
    filename = 'deliverables/ver_{0}/{1}'.format(VERSION, name)

    plot_model(model, to_file=filename)
\end{lstlisting}


\subsection{aux.py}

\begin{lstlisting}[language=python]
#!/usr/bin/python3
# -*- coding:utf-8 -*-

import matplotlib.pyplot as plt

def render_image(image):
    plt.figure()
    plt.imshow(image)
    plt.colorbar()
    plt.grid(False)
    plt.ylabel('Rendered image')
    plt.show()

def render_imagelist(images, labels, class_names):
    plt.figure(figsize=(10,10))
    for i in range(25):
        plt.subplot(5,5,i+1)
        plt.xticks([])
        plt.yticks([])
        plt.grid(False)
        plt.imshow(images[i], cmap=plt.cm.binary)
        plt.xlabel(class_names[labels[i]])
    plt.show()
\end{lstlisting}
